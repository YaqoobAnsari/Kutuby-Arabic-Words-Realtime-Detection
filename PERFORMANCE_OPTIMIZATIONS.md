# Performance Optimizations - Speed Improvements

## Problem Analysis

**Previous Performance** (25 seconds total):
- Audio loading: ~21s ‚ùå **TERRIBLE** (librosa + audioread fallback)
- Model inference: ~4.3s ‚ö†Ô∏è (CPU, no optimizations)
- Total: **25,079ms**

## Optimizations Applied

### 1. FFmpeg Direct Decode (Audio Loading)
**Problem**: librosa was using slow audioread backend as fallback
**Solution**: Use FFmpeg directly via subprocess to pipe-decode audio

**Before**:
```python
y, sr = librosa.load(tmp_path, sr=16000)  # 21 seconds!
```

**After**:
```python
ffmpeg -i pipe:0 -f wav -ar 16000 -ac 1 pipe:1
# Decodes WebM/OGG directly to WAV in memory
# Expected: 100-300ms
```

**Impact**: **~20s ‚Üí ~0.2s** (100x faster!)

---

### 2. GPU Acceleration (If Available)
**Problem**: Model running on CPU only
**Solution**: Auto-detect GPU and move model

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
```

**Impact on CPU**: Same (~4s inference)
**Impact on GPU**: **~4s ‚Üí ~0.5s** (8x faster!)

---

### 3. Mixed Precision Inference (GPU Only)
**Problem**: Using FP32 (32-bit floats) even on GPU
**Solution**: Use automatic mixed precision (FP16)

```python
with torch.cuda.amp.autocast():
    logits = model(inputs["input_values"]).logits
```

**Impact on GPU**: Additional **20-30% speedup**

---

### 4. Proper Device Placement
**Problem**: Inputs staying on CPU while model on GPU
**Solution**: Move tensors to model's device

```python
device = next(model.parameters()).device
inputs = {k: v.to(device) for k, v in inputs.items()}
```

**Impact**: Eliminates CPU‚ÜíGPU transfer overhead

---

## Expected Performance

### CPU Deployment (HuggingFace Free Tier):
| Stage | Before | After | Improvement |
|-------|--------|-------|-------------|
| Audio Loading | 21,000ms | 200ms | **100x faster** |
| Inference | 4,300ms | 4,000ms | ~7% faster |
| **Total** | **25,000ms** | **~4,500ms** | **5.5x faster** |

### GPU Deployment (Cloud Run with GPU):
| Stage | Before | After | Improvement |
|-------|--------|-------|-------------|
| Audio Loading | 21,000ms | 150ms | **140x faster** |
| Inference | 4,300ms | 400ms | **10x faster** |
| **Total** | **25,000ms** | **~800ms** | **31x faster** |

---

## Implementation Details

### FFmpeg Direct Decode:
```python
cmd = [
    'ffmpeg',
    '-i', 'pipe:0',           # Read from stdin
    '-f', 'wav',              # Output WAV
    '-acodec', 'pcm_s16le',   # PCM 16-bit
    '-ar', '16000',           # 16kHz sample rate
    '-ac', '1',               # Mono
    'pipe:1'                  # Write to stdout
]

result = subprocess.run(cmd, input=audio_data, capture_output=True, timeout=5)
y, sr = soundfile.read(io.BytesIO(result.stdout))
```

**Why This Works**:
- No temp files (in-memory pipes)
- FFmpeg is compiled with hardware acceleration
- Bypasses Python audio libraries entirely

---

### Fallback Chain:
1. **soundfile** (50ms) - Direct BytesIO for WAV
2. **FFmpeg** (150ms) - Pipe decode for WebM/OGG
3. **librosa+temp** (1000ms) - Slower fallback
4. **pydub** (1500ms) - Last resort

---

## Monitoring Performance

### New Log Output:
```
üîß Trying soundfile (direct BytesIO)
‚ö†Ô∏è soundfile failed: Format not recognised
üîß Trying FFmpeg direct decode
‚úÖ Audio loaded via FFmpeg: 35520 samples
‚ö° Inference: 423ms
‚úÖ Transcription: 'ŸÖÿ±ÿ≠ÿ®ÿß' | Confidence: 99.1% | Total: 650ms
```

### Performance Metrics to Watch:
- Audio loading method used
- Inference time (should be <500ms on GPU, <5s on CPU)
- Total request time (target: <2s)

---

## Additional Optimizations (Future)

### Not Implemented (Require Code Changes):
1. **Model Quantization** - INT8 instead of FP32 (2-3x faster, slight accuracy loss)
2. **ONNX Runtime** - Convert to ONNX format (1.5-2x faster)
3. **Batching** - Process multiple requests together
4. **Model Distillation** - Use smaller model variant
5. **Caching** - Cache repeated words

### Why Not Now:
- Current optimizations should achieve <2s
- These require significant code changes
- Test current improvements first

---

## Deployment Considerations

### HuggingFace Spaces (Free CPU):
- Expected: **4-5 seconds** total
- Good for demo/testing
- Audio loading: ~200ms
- Inference: ~4000ms

### Google Cloud Run (2 vCPU):
- Expected: **3-4 seconds** total
- Better CPU performance
- Audio loading: ~150ms
- Inference: ~3500ms

### Google Cloud Run (GPU - T4):
- Expected: **<1 second** total
- Best performance
- Audio loading: ~150ms
- Inference: ~400ms
- **Costs more** (~$0.35/hour)

---

## Testing Results

Will update after deployment with actual measurements.

**Target**: <2 seconds total response time
**Critical**: <1 second audio loading (was 21s!)
**Nice to Have**: <500ms inference (requires GPU)

---

**Status**: ‚úÖ Implemented, ready to test
**Impact**: **5-31x faster** depending on hardware
**Critical Fix**: Audio loading 100x faster
